{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "TYPE = 'Type'\n",
    "LOGISTIC = 'logistic'\n",
    "NAIVE_BAYES = 'multinomial naive Bayes'\n",
    "\n",
    "# Get all the tweets.\n",
    "path = r'D:\\Springboard_DataSci\\Twitter_MBTI_predictor\\Data Output'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = [['E', 'I'], ['S', 'N'], ['F', 'T'], ['J', 'P']]\n",
    "MB_types = []\n",
    "# Get the list of types using binary math.\n",
    "for i in range(16):\n",
    "    MB_types.append(letters[0][i//8%2] + letters[1][i//4%2]\n",
    "                      + letters[2][i//2%2] + letters[3][i%2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(MB_type):\n",
    "    return pd.read_csv(\n",
    "        path + '\\\\' + MB_type + '_tweets.csv', parse_dates=[2],\n",
    "        infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets: ESFJ ESFP ESTJ ESTP ENFJ ENFP ENTJ ENTP ISFJ ISFP ISTJ ISTP INFJ INFP INTJ INTP "
     ]
    }
   ],
   "source": [
    "# Load tweets\n",
    "print('Loading tweets:', end=' ')\n",
    "for i, MB_type in enumerate(MB_types):\n",
    "    print(f'{MB_type}', end=' ')\n",
    "    if i == 0:\n",
    "        tweets = load_tweets(MB_type)\n",
    "    else:\n",
    "        tweets = tweets.append(load_tweets(MB_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify their type\n",
    "for i, letter in enumerate('ESFJ'):\n",
    "    tweets[letter] = tweets['MBTI'].str[i] == letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tweets(tweets, letter, classifier, min_df=200, max_df=1.,\n",
    "                   alpha=1., C=1, stop_words=None, get_words_and_probas=False):\n",
    "    '''Text classification of the tweets'''\n",
    "    y = tweets[letter]\n",
    "    vectorizer = CountVectorizer(min_df=min_df, max_df=max_df,\n",
    "                                 stop_words=stop_words)\n",
    "    tweets = tweets['Tweet'].to_list()\n",
    "    # Get the sparse matrix (x, y) of (tweetID, wordID).\n",
    "    X = vectorizer.fit_transform(tweets)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=0)\n",
    "    if classifier==LOGISTIC:\n",
    "        clf = LogisticRegression(C=C, max_iter=1e3, random_state=0)\\\n",
    "            .fit(X_train, y_train)\n",
    "    if classifier==NAIVE_BAYES:\n",
    "        clf = MultinomialNB(alpha=alpha).fit(X_train, y_train)\n",
    "    if get_words_and_probas:\n",
    "        x = np.eye(X_test.shape[1])\n",
    "        words_all = np.array(vectorizer.get_feature_names())\n",
    "        probs = clf.predict_log_proba(x)[:, 0]\n",
    "    else:\n",
    "        words_all = probs = None\n",
    "    return clf.score(X_train, y_train), clf.score(X_test, y_test), words_all,\\\n",
    "        probs\n",
    "        \n",
    "def sort_words_by_coef(words, coefs):\n",
    "    words_order = np.argsort(coefs)\n",
    "    return words[words_order], coefs[words_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing tweets with logistic regression\n",
      "Full set, min_df=10 training and test scores: 0.7019 0.6188\n",
      "Full set, min_df=25 training and test scores: 0.6436 0.5872\n",
      "Full set, min_df=50 training and test scores: 0.6123 0.5701\n",
      "Full set, min_df=100 training and test scores: 0.5874 0.5574\n",
      "Full set, min_df=250 training and test scores: 0.5641 0.5509\n",
      "Full set, min_df=500 training and test scores: 0.5501 0.5441\n"
     ]
    }
   ],
   "source": [
    "print('\\nAnalyzing tweets with logistic regression')\n",
    "for min_df in [10, 25, 50, 100, 250, 500]:\n",
    "    score_base = analyze_tweets(tweets, 'E', classifier=LOGISTIC,\n",
    "                                min_df=min_df)\n",
    "    print(f'Full set, min_df={min_df} training and test scores:', end=' ')\n",
    "    print(round(score_base[0], 4), round(score_base[1], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower min_dfs give better fits but also cause overfitting. Let's retry with\n",
    "multinomial naive Bayes and see what kinds of results we get. We also remove\n",
    "common \"stop\" words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing tweets with naive Bayes and stop words removed\n",
      "No stop words, min_df=10 training and test scores: 0.6734 0.618\n",
      "No stop words, min_df=25 training and test scores: 0.6233 0.5856\n",
      "No stop words, min_df=50 training and test scores: 0.594 0.5668\n",
      "No stop words, min_df=100 training and test scores: 0.5733 0.5557\n",
      "No stop words, min_df=250 training and test scores: 0.5528 0.5473\n",
      "No stop words, min_df=500 training and test scores: 0.5396 0.5359\n"
     ]
    }
   ],
   "source": [
    "print('\\nAnalyzing tweets with naive Bayes and stop words removed')\n",
    "for min_df in [10, 25, 50, 100, 250, 500]:\n",
    "    score_base = analyze_tweets(tweets, 'E', classifier=NAIVE_BAYES,\n",
    "                                min_df=min_df, stop_words=ENGLISH_STOP_WORDS)\n",
    "    print(f'No stop words, min_df={min_df} training and test scores:', end=' ')\n",
    "    print(round(score_base[0], 4), round(score_base[1], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores are marginally hurt, but we have fewer features to worry about.\n",
    "This is an acceptable trade. Let's see what the features are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes, stop words removed, min_df=10\n",
      "Scores: 0.6734 0.618\n"
     ]
    }
   ],
   "source": [
    "min_df=10\n",
    "print(f'\\nNaive Bayes, stop words removed, min_df={min_df}')\n",
    "bayes_results = analyze_tweets(\n",
    "    tweets, 'E', classifier=NAIVE_BAYES, min_df=min_df,\n",
    "    stop_words=ENGLISH_STOP_WORDS, get_words_and_probas=True)\n",
    "print('Scores:', round(bayes_results[0], 4), round(bayes_results[1], 4))\n",
    "nb_words = bayes_results[2]\n",
    "nb_coefs = bayes_results[3]\n",
    "nb_words, nb_coefs = sort_words_by_coef(nb_words, nb_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these word are nonsense, probably other users. We need to rerun\n",
    "this with a much higher min_df, even at the cost of scores. Let's also lower\n",
    "the value of alpha in the naive Bayes to get stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes, stop words removed, min_df=200, alpha=5\n",
      "Scores: 0.5567 0.549\n"
     ]
    }
   ],
   "source": [
    "min_df=200; alpha=5\n",
    "print(f'\\nNaive Bayes, stop words removed, min_df={min_df}, alpha={alpha}')\n",
    "bayes_results = analyze_tweets(\n",
    "    tweets, 'E', classifier=NAIVE_BAYES, min_df=min_df, alpha=alpha,\n",
    "    stop_words=ENGLISH_STOP_WORDS, get_words_and_probas=True)\n",
    "print('Scores:', round(bayes_results[0], 4), round(bayes_results[1], 4))\n",
    "nb_words = bayes_results[2]\n",
    "nb_coefs = bayes_results[3]\n",
    "nb_words, nb_coefs = sort_words_by_coef(nb_words, nb_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps returning the stop words and lowering min_df will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes, stop words returned, min_df=100, alpha=5\n",
      "Scores: 0.576 0.5612\n"
     ]
    }
   ],
   "source": [
    "min_df=100; alpha=5\n",
    "print(f'\\nNaive Bayes, stop words returned, min_df={min_df}, alpha={alpha}')\n",
    "bayes_results = analyze_tweets(\n",
    "    tweets, 'E', classifier=NAIVE_BAYES, min_df=min_df, alpha=alpha,\n",
    "    get_words_and_probas=True)\n",
    "print('Scores:', round(bayes_results[0], 4), round(bayes_results[1], 4))\n",
    "nb_words = bayes_results[2]\n",
    "nb_coefs = bayes_results[3]\n",
    "nb_words, nb_coefs = sort_words_by_coef(nb_words, nb_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results leave a lot to be desired. One possibility is that when one\n",
    "author tweets a lot of words, they unduly weight particular words in their\n",
    "favor. Let's combine tweets per author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_author = tweets.copy()\n",
    "tweets_per_author['Tweet'] = tweets_per_author['Tweet']\\\n",
    "    .apply(lambda x: x + ' ')\n",
    "tweets_per_author = tweets_per_author.groupby(\n",
    "    tweets_per_author['Screen name'])['Tweet'].apply(lambda x: x.sum())\\\n",
    "    .reset_index()\n",
    "# This threw away the MBTI info, but we can get it back.\n",
    "authors_MBTI = tweets[['Screen name', 'E', 'S', 'F', 'J']].drop_duplicates()\n",
    "tweets_per_author = tweets_per_author.merge(\n",
    "    authors_MBTI, 'left', on='Screen name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rerun with combined tweets. We try a lower min_df because of the\n",
    "combined words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes, min_df=50\n",
      "Scores: 0.7883 0.595\n"
     ]
    }
   ],
   "source": [
    "min_df=50\n",
    "print(f'\\nNaive Bayes, min_df={min_df}')\n",
    "bayes_results = analyze_tweets(\n",
    "    tweets_per_author, 'E', classifier=NAIVE_BAYES, min_df=min_df,\n",
    "    get_words_and_probas=True)\n",
    "print('Scores:', round(bayes_results[0], 4), round(bayes_results[1], 4))\n",
    "nb_words = bayes_results[2]\n",
    "nb_coefs = bayes_results[3]\n",
    "nb_words, nb_coefs = sort_words_by_coef(nb_words, nb_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Badly overfit. Let's change min_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes, stop words removed, min_df=200\n",
      "Scores: 0.7167 0.595\n"
     ]
    }
   ],
   "source": [
    "min_df=200\n",
    "print(f'\\nNaive Bayes, stop words removed, min_df={min_df}')\n",
    "bayes_results = analyze_tweets(\n",
    "    tweets_per_author, 'E', classifier=NAIVE_BAYES, min_df=min_df,\n",
    "    get_words_and_probas=True)\n",
    "print('Scores:', round(bayes_results[0], 4), round(bayes_results[1], 4))\n",
    "nb_words = bayes_results[2]\n",
    "nb_coefs = bayes_results[3]\n",
    "nb_words, nb_coefs = sort_words_by_coef(nb_words, nb_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less bad but still problematic. Let's experiment with alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes, min_df=200, alpha=10\n",
      "Scores: 0.7183 0.6\n"
     ]
    }
   ],
   "source": [
    "min_df=200; alpha=10\n",
    "print(f'\\nNaive Bayes, min_df={min_df}, alpha={alpha}')\n",
    "bayes_results = analyze_tweets(\n",
    "    tweets_per_author, 'E', classifier=NAIVE_BAYES, min_df=min_df, alpha=alpha,\n",
    "    get_words_and_probas=True)\n",
    "print('Scores:', round(bayes_results[0], 4), round(bayes_results[1], 4))\n",
    "nb_words = bayes_results[2]\n",
    "nb_coefs = bayes_results[3]\n",
    "nb_words, nb_coefs = sort_words_by_coef(nb_words, nb_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes, min_df=200, alpha=0.1\n",
      "Scores: 0.7158 0.595\n"
     ]
    }
   ],
   "source": [
    "min_df=200; alpha=0.1\n",
    "print(f'\\nNaive Bayes, min_df={min_df}, alpha={alpha}')\n",
    "bayes_results = analyze_tweets(\n",
    "    tweets_per_author, 'E', classifier=NAIVE_BAYES, min_df=min_df, alpha=alpha,\n",
    "    get_words_and_probas=True)\n",
    "print('Scores:', round(bayes_results[0], 4), round(bayes_results[1], 4))\n",
    "nb_words = bayes_results[2]\n",
    "nb_coefs = bayes_results[3]\n",
    "nb_words, nb_coefs = sort_words_by_coef(nb_words, nb_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps min_df is an issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes, min_df=500\n",
      "Scores: 0.66 0.5975\n"
     ]
    }
   ],
   "source": [
    "min_df=500\n",
    "print(f'\\nNaive Bayes, min_df={min_df}')\n",
    "bayes_results = analyze_tweets(\n",
    "    tweets_per_author, 'E', classifier=NAIVE_BAYES, min_df=min_df,\n",
    "    get_words_and_probas=True)\n",
    "print('Scores:', round(bayes_results[0], 4), round(bayes_results[1], 4))\n",
    "nb_words = bayes_results[2]\n",
    "nb_coefs = bayes_results[3]\n",
    "nb_words, nb_coefs = sort_words_by_coef(nb_words, nb_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes, min_df=250\n",
      "Scores: 0.6542 0.675\n"
     ]
    }
   ],
   "source": [
    "min_df=250\n",
    "print(f'\\nNaive Bayes, min_df={min_df}')\n",
    "bayes_results = analyze_tweets(\n",
    "    tweets_per_author, 'S', classifier=NAIVE_BAYES, min_df=min_df,\n",
    "    get_words_and_probas=True)\n",
    "print('Scores:', round(bayes_results[0], 4), round(bayes_results[1], 4))\n",
    "nb_words = bayes_results[2]\n",
    "nb_coefs = bayes_results[3]\n",
    "nb_words, nb_coefs = sort_words_by_coef(nb_words, nb_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_use(words, tweets_with_words, trait, other_trait):\n",
    "    '''Split totals of unique authors who tweeted these words.\n",
    "    Trait needs to be E/S/F/J; other_trait, I/N/T/P.'''\n",
    "    unique_words = pd.DataFrame(columns=['Word', trait, other_trait])\n",
    "    for i, word in enumerate(words):\n",
    "        # Column 1: Whether each author tweeted the word of interest.\n",
    "        word_in_tweets = tweets_per_author['Tweet'].str.contains(word)\\\n",
    "            .to_frame()\n",
    "        # Column 2: Whether that author has the trait.\n",
    "        word_in_tweets[trait] = tweets_per_author[trait].to_numpy()\n",
    "        # Now get the summaries. Set the index to the trait boolean\n",
    "        word_use = word_in_tweets.value_counts().reset_index()\\\n",
    "            .set_index(trait)\n",
    "        # Only keep the ones with the word actually in them\n",
    "        word_use = word_use[word_use['Tweet']]\n",
    "        # Add this info to unique_words\n",
    "        unique_words.loc[i] = [word, word_use.loc[True, 0],\n",
    "                               word_use.loc[False, 0]]\n",
    "    # Column 1 is the trait; column 2 is the \"not trait.\"\n",
    "    unique_words['Percent ' + trait] = 100*unique_words[trait] /\\\n",
    "        (unique_words[trait]+unique_words[other_trait])\n",
    "    unique_words.sort_values('Percent ' + trait, inplace=True)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating unique words\n",
      "          Word    E    I Percent E\n",
      "680     anyway  136  189   41.8462\n",
      "218   probably  228  311   42.3006\n",
      "26        lmao   92  123   42.7907\n",
      "490  character  176  226   43.7811\n",
      "756    writing  133  169   44.0397\n",
      "..         ...  ...  ...       ...\n",
      "125      girls  150  113   57.0342\n",
      "82     service  210  158   57.0652\n",
      "736      local  164  116   58.5714\n",
      "45      coffee  173  119   59.2466\n",
      "621    sharing  175  117   59.9315\n",
      "\n",
      "[760 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print('Calculating unique words')\n",
    "unique_words_EI = get_word_use(nb_words, tweets_per_author, trait='E',\n",
    "                               other_trait='I')\n",
    "print(unique_words_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word    S    N Percent S\n",
      "755      truth  114  175   39.4464\n",
      "746  important  174  266   39.5455\n",
      "757      trump   39   59   39.7959\n",
      "735    history  107  158   40.3774\n",
      "742   children  103  148   41.0359\n",
      "..         ...  ...  ...       ...\n",
      "176     friday   28   20   58.3333\n",
      "11       kinda  178  122   59.3333\n",
      "26        lmao  131   84   60.9302\n",
      "29         wtf  102   61   62.5767\n",
      "7          idk  139   81   63.1818\n",
      "\n",
      "[760 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "unique_words_SN = get_word_use(nb_words, tweets_per_author, trait='S',\n",
    "                               other_trait='N')\n",
    "print(unique_words_SN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word    F    T Percent F\n",
      "754   american   18   32        36\n",
      "176     friday   19   29   39.5833\n",
      "87       level  155  220   41.3333\n",
      "533     office  146  204   41.7143\n",
      "643  attention  109  149   42.2481\n",
      "..         ...  ...  ...       ...\n",
      "638  wonderful  149  104   58.8933\n",
      "8         baby  264  180   59.4595\n",
      "29         wtf   98   65   60.1227\n",
      "197      sweet  267  173   60.6818\n",
      "131        omg  161   95   62.8906\n",
      "\n",
      "[760 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "unique_words_FT = get_word_use(nb_words, tweets_per_author, trait='F',\n",
    "                               other_trait='T')\n",
    "print(unique_words_FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word    J    P Percent J\n",
      "7        idk   66  154        30\n",
      "176   friday   16   32   33.3333\n",
      "26      lmao   72  143   33.4884\n",
      "569  youtube   20   39   33.8983\n",
      "29       wtf   57  106   34.9693\n",
      "..       ...  ...  ...       ...\n",
      "660   system  193  140    57.958\n",
      "87     level  220  155   58.6667\n",
      "736    local  165  115   58.9286\n",
      "679    books  178  122   59.3333\n",
      "621  sharing  178  114   60.9589\n",
      "\n",
      "[760 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "unique_words_JP = get_word_use(nb_words, tweets_per_author, trait='J',\n",
    "                               other_trait='P')\n",
    "print(unique_words_JP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some percentages farther away from 50 start to show up. Perhaps we should\n",
    "lower min_df after all?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
