{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "TYPE = 'Type'\n",
    "LOGISTIC = 'logistic'\n",
    "NAIVE_BAYES = 'multinomial naive Bayes'\n",
    "\n",
    "# Get all the tweets.\n",
    "path = r'D:\\Springboard_DataSci\\Twitter_MBTI_predictor\\Data Output'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = [['E', 'I'], ['S', 'N'], ['F', 'T'], ['J', 'P']]\n",
    "MB_types = []\n",
    "# Get the list of types using binary math.\n",
    "for i in range(16):\n",
    "    MB_types.append(letters[0][i//8%2] + letters[1][i//4%2]\n",
    "                      + letters[2][i//2%2] + letters[3][i%2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(MB_type):\n",
    "    return pd.read_csv(\n",
    "        path + '\\\\' + MB_type + '_tweets.csv', parse_dates=[2],\n",
    "        infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets: ESFJ ESFP ESTJ ESTP ENFJ ENFP ENTJ ENTP ISFJ ISFP ISTJ ISTP INFJ INFP INTJ INTP "
     ]
    }
   ],
   "source": [
    "# Load tweets\n",
    "print('Loading tweets:', end=' ')\n",
    "for i, MB_type in enumerate(MB_types):\n",
    "    print(f'{MB_type}', end=' ')\n",
    "    if i == 0:\n",
    "        tweets = load_tweets(MB_type)\n",
    "    else:\n",
    "        tweets = tweets.append(load_tweets(MB_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify their type\n",
    "for i, letter in enumerate('ESFJ'):\n",
    "    tweets[letter] = tweets['MBTI'].str[i] == letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pick up a lot of junk if we don't trim out tags and hashtags.\n",
    "Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\n",
    "                           \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trimming tweets of tags and URLs\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrimming tweets of tags and URLs')\n",
    "tweets['Tweet'] = tweets['Tweet'].apply(trim_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's attempt to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tweets(tweets, letter, classifier, min_df=200, max_df=1.,\n",
    "                   alpha=1., C=1, stop_words=None, get_words_and_probas=False,\n",
    "                   test_size=0.25, max_iter=1e3):\n",
    "    '''Text classification of the tweets'''\n",
    "    y = tweets[letter]\n",
    "    vectorizer = CountVectorizer(min_df=min_df, max_df=max_df,\n",
    "                                 stop_words=stop_words)\n",
    "    tweets = tweets['Tweet'].to_list()\n",
    "    # Get the sparse matrix (x, y) of (tweetID, wordID).\n",
    "    X = vectorizer.fit_transform(tweets)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0)\n",
    "    if classifier==LOGISTIC:\n",
    "        clf = LogisticRegression(C=C, max_iter=max_iter, random_state=0)\\\n",
    "            .fit(X_train, y_train)\n",
    "    if classifier==NAIVE_BAYES:\n",
    "        clf = MultinomialNB(alpha=alpha).fit(X_train, y_train)\n",
    "    if get_words_and_probas:\n",
    "        x = np.eye(X_test.shape[1])\n",
    "        words_all = np.array(vectorizer.get_feature_names())\n",
    "        probs = clf.predict_log_proba(x)[:, 0]\n",
    "    else:\n",
    "        words_all = probs = None\n",
    "    return clf.score(X_train, y_train), clf.score(X_test, y_test), words_all,\\\n",
    "        probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping tweets by author\n"
     ]
    }
   ],
   "source": [
    "print('Grouping tweets by author')\n",
    "tweets_per_author = tweets.copy()\n",
    "tweets_per_author['Tweet'] = tweets_per_author['Tweet']\\\n",
    "    .apply(lambda x: x + ' ')\n",
    "tweets_per_author = tweets_per_author.groupby(\n",
    "    tweets_per_author['Screen name'])['Tweet'].apply(lambda x: x.sum())\\\n",
    "    .reset_index()\n",
    "# This threw away the MBTI info, but we can get it back.\n",
    "authors_MBTI = tweets[['Screen name', 'E', 'S', 'F', 'J']].drop_duplicates()\n",
    "tweets_per_author = tweets_per_author.merge(\n",
    "    authors_MBTI, 'left', on='Screen name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've looked through several combinations of hyperparameters. Let's look\n",
    "for the one that performs the best. First we do the E/I axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing tweets at the author level: E/I axis\n",
      "\tTesting with a min_df of 100\n",
      "\tTesting with a min_df of 150\n",
      "\tTesting with a min_df of 200\n",
      "\tTesting with a min_df of 300\n",
      "\tTesting with a min_df of 400\n",
      "\tTesting with a min_df of 500\n",
      "Best min_df, test size, and score: 300 0.25 0.605\n"
     ]
    }
   ],
   "source": [
    "print('\\nAnalyzing tweets at the author level: E/I axis')\n",
    "best_min_df, best_test_size, best_test_score = 0, 0, 0\n",
    "for min_df in [100, 150, 200, 300, 400, 500]:\n",
    "    print(f'\\tTesting with a min_df of {min_df}')\n",
    "    for test_size in [0.2, 0.25, 0.3, 0.35, 0.4]:\n",
    "        author_results = analyze_tweets(\n",
    "            tweets_per_author, 'E', classifier=NAIVE_BAYES, min_df=min_df,\n",
    "            get_words_and_probas=True, test_size=test_size)\n",
    "        train_score, test_score = round(author_results[0], 4),\\\n",
    "            round(author_results[1], 4)\n",
    "        if test_score > best_test_score:\n",
    "            best_min_df, best_test_size, best_test_score = min_df, test_size,\\\n",
    "                test_score\n",
    "                \n",
    "print('Best min_df, test size, and score:',\n",
    "      best_min_df, best_test_size, best_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are: min_df=300, test_size=0.25, and test_score=0.605.\n",
    "<br>Next, the S/N axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing tweets at the author level: S/N axis\n",
      "\tTesting with a min_df of 100\n",
      "\tTesting with a min_df of 150\n",
      "\tTesting with a min_df of 200\n",
      "\tTesting with a min_df of 300\n",
      "\tTesting with a min_df of 400\n",
      "\tTesting with a min_df of 500\n",
      "Best min_df, test size, and score: 150 0.25 0.665\n"
     ]
    }
   ],
   "source": [
    "print('\\nAnalyzing tweets at the author level: S/N axis')\n",
    "best_min_df, best_test_size, best_test_score = 0, 0, 0\n",
    "for min_df in [100, 150, 200, 300, 400, 500]:\n",
    "    print(f'\\tTesting with a min_df of {min_df}')\n",
    "    for test_size in [0.2, 0.25, 0.3, 0.35, 0.4]:\n",
    "        author_results = analyze_tweets(\n",
    "            tweets_per_author, 'S', classifier=NAIVE_BAYES, min_df=min_df,\n",
    "            get_words_and_probas=True, test_size=test_size)\n",
    "        train_score, test_score = round(author_results[0], 4),\\\n",
    "            round(author_results[1], 4)\n",
    "        if test_score > best_test_score:\n",
    "            best_min_df, best_test_size, best_test_score = min_df, test_size,\\\n",
    "                test_score\n",
    "                \n",
    "print('Best min_df, test size, and score:',\n",
    "      best_min_df, best_test_size, best_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are: min_df=150, test_size=0.25, and test_score=0.665.\n",
    "<br>Now the F/T axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing tweets at the author level: F/T axis\n",
      "\tTesting with a min_df of 100\n",
      "\tTesting with a min_df of 150\n",
      "\tTesting with a min_df of 200\n",
      "\tTesting with a min_df of 300\n",
      "\tTesting with a min_df of 400\n",
      "\tTesting with a min_df of 500\n",
      "Best min_df, test size, and score: 500 0.2 0.6094\n"
     ]
    }
   ],
   "source": [
    "print('\\nAnalyzing tweets at the author level: F/T axis')\n",
    "best_min_df, best_test_size, best_test_score = 0, 0, 0\n",
    "for min_df in [100, 150, 200, 300, 400, 500]:\n",
    "    print(f'\\tTesting with a min_df of {min_df}')\n",
    "    for test_size in [0.2, 0.25, 0.3, 0.35, 0.4]:\n",
    "        author_results = analyze_tweets(\n",
    "            tweets_per_author, 'F', classifier=NAIVE_BAYES, min_df=min_df,\n",
    "            get_words_and_probas=True, test_size=test_size)\n",
    "        train_score, test_score = round(author_results[0], 4),\\\n",
    "            round(author_results[1], 4)\n",
    "        if test_score > best_test_score:\n",
    "            best_min_df, best_test_size, best_test_score = min_df, test_size,\\\n",
    "                test_score\n",
    "                \n",
    "print('Best min_df, test size, and score:',\n",
    "      best_min_df, best_test_size, best_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are: min_df=500, test_size=0.2, and test_score=0.6094.\n",
    "<br>Finally, the J/P axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing tweets at the author level: J/P axis\n",
      "\tTesting with a min_df of 100\n",
      "\tTesting with a min_df of 150\n",
      "\tTesting with a min_df of 200\n",
      "\tTesting with a min_df of 300\n",
      "\tTesting with a min_df of 400\n",
      "\tTesting with a min_df of 500\n",
      "Best min_df, test size, and score: 100 0.4 0.5828\n"
     ]
    }
   ],
   "source": [
    "print('\\nAnalyzing tweets at the author level: J/P axis')\n",
    "best_min_df, best_test_size, best_test_score = 0, 0, 0\n",
    "for min_df in [100, 150, 200, 300, 400, 500]:\n",
    "    print(f'\\tTesting with a min_df of {min_df}')\n",
    "    for test_size in [0.2, 0.25, 0.3, 0.35, 0.4]:\n",
    "        author_results = analyze_tweets(\n",
    "            tweets_per_author, 'J', classifier=NAIVE_BAYES, min_df=min_df,\n",
    "            get_words_and_probas=True, test_size=test_size)\n",
    "        train_score, test_score = round(author_results[0], 4),\\\n",
    "            round(author_results[1], 4)\n",
    "        if test_score > best_test_score:\n",
    "            best_min_df, best_test_size, best_test_score = min_df, test_size,\\\n",
    "                test_score\n",
    "                \n",
    "print('Best min_df, test size, and score:',\n",
    "      best_min_df, best_test_size, best_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are: min_df=100, test_size=0.4, and test_score=0.5828."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
